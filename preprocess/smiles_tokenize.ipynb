{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07fd11c2-6436-4800-bf72-6ec2dda52d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import atomInSmiles\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "545f74bf-fdd4-49cd-b229-2579f654e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet('/data/datasets/leash-BELKA/random_stratified_split/train.parquet')\n",
    "df_valid = pd.read_parquet('/data/datasets/leash-BELKA/random_stratified_split/valid.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29d64af-d40e-4ddd-980f-a4ce86008caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet('/data/datasets/leash-BELKA/origin/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90bb1b1f-b81b-445e-b145-36ef6821131b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>buildingblock1_smiles</th>\n",
       "      <th>buildingblock2_smiles</th>\n",
       "      <th>buildingblock3_smiles</th>\n",
       "      <th>molecule_smiles</th>\n",
       "      <th>protein_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>BRD4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>HSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>sEH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833</td>\n",
       "      <td>C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O</td>\n",
       "      <td>C=Cc1ccc(N)cc1</td>\n",
       "      <td>CC(O)Cn1cnc2c(N)ncnc21</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...</td>\n",
       "      <td>BRD4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                            buildingblock1_smiles   \n",
       "0  295246830  C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O  \\\n",
       "1  295246831  C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "2  295246832  C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "3  295246833  C#CCCC[C@H](NC(=O)OCC1c2ccccc2-c2ccccc21)C(=O)O   \n",
       "\n",
       "  buildingblock2_smiles   buildingblock3_smiles   \n",
       "0        C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1  \\\n",
       "1        C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "2        C=Cc1ccc(N)cc1          C=Cc1ccc(N)cc1   \n",
       "3        C=Cc1ccc(N)cc1  CC(O)Cn1cnc2c(N)ncnc21   \n",
       "\n",
       "                                     molecule_smiles protein_name  \n",
       "0  C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...         BRD4  \n",
       "1  C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...          HSA  \n",
       "2  C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...          sEH  \n",
       "3  C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...         BRD4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5667d62e-5d1b-4079-bedd-ac7b2a6c731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/datasets/leash-BELKA/AIS-Token-Dict.pickle', 'rb') as f:\n",
    "    ais_tokens_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a040b4f-3f67-4509-8537-5d75071e9110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#': 1,\n",
       " '(': 2,\n",
       " ')': 3,\n",
       " '-': 4,\n",
       " '/': 5,\n",
       " '1': 6,\n",
       " '2': 7,\n",
       " '3': 8,\n",
       " '4': 9,\n",
       " '5': 10,\n",
       " '6': 11,\n",
       " '7': 12,\n",
       " '8': 13,\n",
       " '9': 14,\n",
       " '=': 15,\n",
       " '[B;R;COO]': 16,\n",
       " '[Br;!R;C]': 17,\n",
       " '[C;!R;CCCC]': 18,\n",
       " '[C;!R;CCCN]': 19,\n",
       " '[C;!R;CCCO]': 20,\n",
       " '[C;!R;CCC]': 21,\n",
       " '[C;!R;CCCl]': 22,\n",
       " '[C;!R;CCFF]': 23,\n",
       " '[C;!R;CCOO]': 24,\n",
       " '[C;!R;CCO]': 25,\n",
       " '[C;!R;CC]': 26,\n",
       " '[C;!R;CClCl]': 27,\n",
       " '[C;!R;CFFF]': 28,\n",
       " '[C;!R;CNN]': 29,\n",
       " '[C;!R;CNO]': 30,\n",
       " '[C;!R;CNS]': 31,\n",
       " '[C;!R;CN]': 32,\n",
       " '[C;!R;COO]': 33,\n",
       " '[C;!R;COS]': 34,\n",
       " '[C;!R;CSi]': 35,\n",
       " '[C;!R;FFFO]': 36,\n",
       " '[C;!R;NNO]': 37,\n",
       " '[C;!R;NOO]': 38,\n",
       " '[C;R;CCCC]': 39,\n",
       " '[C;R;CCCF]': 40,\n",
       " '[C;R;CCCN]': 41,\n",
       " '[C;R;CCCO]': 42,\n",
       " '[C;R;CCCS]': 43,\n",
       " '[C;R;CCC]': 44,\n",
       " '[C;R;CCFF]': 45,\n",
       " '[C;R;CCN]': 46,\n",
       " '[C;R;CCOO]': 47,\n",
       " '[C;R;CCO]': 48,\n",
       " '[C;R;CCS]': 49,\n",
       " '[C;R;CNN]': 50,\n",
       " '[C;R;CNO]': 51,\n",
       " '[C;R;COO]': 52,\n",
       " '[C;R;FFOO]': 53,\n",
       " '[C;R;NNO]': 54,\n",
       " '[C;R;NNS]': 55,\n",
       " '[C;R;NOO]': 56,\n",
       " '[C;R;NOS]': 57,\n",
       " '[CH2;!R;CC]': 58,\n",
       " '[CH2;!R;CF]': 59,\n",
       " '[CH2;!R;CN]': 60,\n",
       " '[CH2;!R;CO]': 61,\n",
       " '[CH2;!R;CS]': 62,\n",
       " '[CH2;!R;C]': 63,\n",
       " '[CH2;!R;NN]': 64,\n",
       " '[CH2;!R;NO]': 65,\n",
       " '[CH2;!R;OO]': 66,\n",
       " '[CH2;R;CC]': 67,\n",
       " '[CH2;R;CN]': 68,\n",
       " '[CH2;R;CO]': 69,\n",
       " '[CH2;R;CS]': 70,\n",
       " '[CH2;R;NS]': 71,\n",
       " '[CH2;R;OO]': 72,\n",
       " '[CH3;!R;C]': 73,\n",
       " '[CH3;!R;N]': 74,\n",
       " '[CH3;!R;O]': 75,\n",
       " '[CH3;!R;S]': 76,\n",
       " '[CH3;!R;Si]': 77,\n",
       " '[CH;!R;BrC]': 78,\n",
       " '[CH;!R;CCC]': 79,\n",
       " '[CH;!R;CCF]': 80,\n",
       " '[CH;!R;CCN]': 81,\n",
       " '[CH;!R;CCO]': 82,\n",
       " '[CH;!R;CCS]': 83,\n",
       " '[CH;!R;CC]': 84,\n",
       " '[CH;!R;CFF]': 85,\n",
       " '[CH;!R;CO]': 86,\n",
       " '[CH;!R;C]': 87,\n",
       " '[CH;!R;FFN]': 88,\n",
       " '[CH;!R;FFO]': 89,\n",
       " '[CH;R;CCC]': 90,\n",
       " '[CH;R;CCF]': 91,\n",
       " '[CH;R;CCN]': 92,\n",
       " '[CH;R;CCO]': 93,\n",
       " '[CH;R;CCS]': 94,\n",
       " '[CH;R;CC]': 95,\n",
       " '[CH;R;CNN]': 96,\n",
       " '[CH;R;CNO]': 97,\n",
       " '[CH;R;CN]': 98,\n",
       " '[CH;R;CO]': 99,\n",
       " '[CH;R;CSS]': 100,\n",
       " '[Cl;!R;C]': 101,\n",
       " '[F;!R;C]': 102,\n",
       " '[I;!R;C]': 103,\n",
       " '[N;!R;CCC]': 104,\n",
       " '[N;!R;CCO]': 105,\n",
       " '[N;!R;CCS]': 106,\n",
       " '[N;!R;CN]': 107,\n",
       " '[N;!R;C]': 108,\n",
       " '[N;R;CCC]': 109,\n",
       " '[N;R;CCN]': 110,\n",
       " '[N;R;CCO]': 111,\n",
       " '[N;R;CCS]': 112,\n",
       " '[N;R;CC]': 113,\n",
       " '[N;R;CN]': 114,\n",
       " '[N;R;CO]': 115,\n",
       " '[NH2;!R;C]': 116,\n",
       " '[NH2;!R;S]': 117,\n",
       " '[NH;!R;CC]': 118,\n",
       " '[NH;!R;CS]': 119,\n",
       " '[NH;!R;C]': 120,\n",
       " '[NH;R;CC]': 121,\n",
       " '[O;!R;CC]': 122,\n",
       " '[O;!R;CN]': 123,\n",
       " '[O;!R;C]': 124,\n",
       " '[O;!R;N]': 125,\n",
       " '[O;!R;S]': 126,\n",
       " '[O;R;BC]': 127,\n",
       " '[O;R;CC]': 128,\n",
       " '[O;R;CN]': 129,\n",
       " '[OH;!R;C]': 130,\n",
       " '[S;!R;CCOO]': 131,\n",
       " '[S;!R;CCO]': 132,\n",
       " '[S;!R;CC]': 133,\n",
       " '[S;!R;CNOO]': 134,\n",
       " '[S;!R;CS]': 135,\n",
       " '[S;!R;C]': 136,\n",
       " '[S;!R;NNOO]': 137,\n",
       " '[S;R;CCOO]': 138,\n",
       " '[S;R;CCO]': 139,\n",
       " '[S;R;CC]': 140,\n",
       " '[S;R;CNOO]': 141,\n",
       " '[SH;!R;C]': 142,\n",
       " '[Si;!R;CCCC]': 143,\n",
       " '[[C@@H];!R;CCN]': 144,\n",
       " '[[C@@H];!R;CCO]': 145,\n",
       " '[[C@@H];R;CCC]': 146,\n",
       " '[[C@@H];R;CCF]': 147,\n",
       " '[[C@@H];R;CCN]': 148,\n",
       " '[[C@@H];R;CCO]': 149,\n",
       " '[[C@@];!R;CCCN]': 150,\n",
       " '[[C@@];R;CCCC]': 151,\n",
       " '[[C@@];R;CCCN]': 152,\n",
       " '[[C@H];!R;CCN]': 153,\n",
       " '[[C@H];!R;CCO]': 154,\n",
       " '[[C@H];R;CCC]': 155,\n",
       " '[[C@H];R;CCF]': 156,\n",
       " '[[C@H];R;CCN]': 157,\n",
       " '[[C@H];R;CCO]': 158,\n",
       " '[[C@];!R;CCCN]': 159,\n",
       " '[[C@];R;CCCC]': 160,\n",
       " '[[C@];R;CCCN]': 161,\n",
       " '[[N+];!R;COO]': 162,\n",
       " '[[N+];!R;NN]': 163,\n",
       " '[[N-];!R;N]': 164,\n",
       " '[[O-];!R;N]': 165,\n",
       " '[[n+];R;CCO]': 166,\n",
       " '[c;R;BCC]': 167,\n",
       " '[c;R;BrCC]': 168,\n",
       " '[c;R;BrCN]': 169,\n",
       " '[c;R;BrCS]': 170,\n",
       " '[c;R;CCC]': 171,\n",
       " '[c;R;CCCl]': 172,\n",
       " '[c;R;CCF]': 173,\n",
       " '[c;R;CCI]': 174,\n",
       " '[c;R;CCN]': 175,\n",
       " '[c;R;CCO]': 176,\n",
       " '[c;R;CCS]': 177,\n",
       " '[c;R;CClN]': 178,\n",
       " '[c;R;CClS]': 179,\n",
       " '[c;R;CFN]': 180,\n",
       " '[c;R;CNN]': 181,\n",
       " '[c;R;CNO]': 182,\n",
       " '[c;R;CNS]': 183,\n",
       " '[c;R;COO]': 184,\n",
       " '[c;R;COS]': 185,\n",
       " '[c;R;CSS]': 186,\n",
       " '[c;R;ClNN]': 187,\n",
       " '[c;R;ClNS]': 188,\n",
       " '[c;R;FNN]': 189,\n",
       " '[c;R;NNN]': 190,\n",
       " '[c;R;NNO]': 191,\n",
       " '[c;R;NNS]': 192,\n",
       " '[c;R;NOO]': 193,\n",
       " '[c;R;NOS]': 194,\n",
       " '[c;R;NSS]': 195,\n",
       " '[cH;R;CC]': 196,\n",
       " '[cH;R;CN]': 197,\n",
       " '[cH;R;CO]': 198,\n",
       " '[cH;R;CS]': 199,\n",
       " '[cH;R;NN]': 200,\n",
       " '[cH;R;NO]': 201,\n",
       " '[cH;R;NS]': 202,\n",
       " '[n;R;CCC]': 203,\n",
       " '[n;R;CCN]': 204,\n",
       " '[n;R;CC]': 205,\n",
       " '[n;R;CNN]': 206,\n",
       " '[n;R;CN]': 207,\n",
       " '[n;R;CO]': 208,\n",
       " '[n;R;CS]': 209,\n",
       " '[n;R;NN]': 210,\n",
       " '[n;R;NS]': 211,\n",
       " '[nH;R;CC]': 212,\n",
       " '[nH;R;CN]': 213,\n",
       " '[nH;R;NN]': 214,\n",
       " '[o;R;CC]': 215,\n",
       " '[o;R;CN]': 216,\n",
       " '[o;R;NN]': 217,\n",
       " '[s;R;CC]': 218,\n",
       " '[s;R;CN]': 219,\n",
       " '[s;R;NN]': 220}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ais_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb5a1f6e-537e-4228-b31e-23f9573d367c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = atomInSmiles.similarity('[s;R;CN]', '[o;R;CN]')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd0cff7-6bda-47ba-b98f-12970222f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "PAD = 0\n",
    "UNK = 221\n",
    "\n",
    "def remove_dy(smiles):\n",
    "    return smiles.replace(\"[Dy]\", \"\")\n",
    "\n",
    "def tokenize(smiles):\n",
    "    smiles = remove_dy(smiles)\n",
    "    ais_tokens = atomInSmiles.encode(smiles)\n",
    "    \n",
    "    return ais_tokens.split()\n",
    "\n",
    "def encode_smile(smiles):\n",
    "    tokens = tokenize(smiles)\n",
    "    tmp = [ais_tokens_dict[tk] for tk in tokens]\n",
    "    assert len(tmp) <= MAX_LEN, \"Max length overhead\"\n",
    "    tmp = tmp + [PAD]*(MAX_LEN-len(tmp))\n",
    "    return np.array(tmp).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "734d6312-f780-4f07-ae85-626c6ded9db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116  60  33   2  15 124   3 130   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "smiles = 'NCC(=O)O'\n",
    "\n",
    "print(encode_smile(smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d0c7754-ac17-4ddf-834f-54eabff0c1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1674896"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_test = df_test['molecule_smiles'].values\n",
    "len(smiles_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e46391c-a6b7-4369-a36e-8ed20af3fc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 50, 53, 64, 65, 80, 91, 98, 99, 137, 151, 160, 186, 193]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data/datasets/leash-BELKA/AIS-UNK-Tokens.pickle', 'rb') as f:\n",
    "    UNK_tokens = pickle.load(f)\n",
    "UNK_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc57f7b4-9ff1-4a39-be97-65056c3e4086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 50, 53, 64, 65, 80, 91, 98, 99, 137, 151, 160, 186, 193]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNK_tokens = [v + 1 for v in UNK_tokens]\n",
    "# UNK_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5952d850-eb8d-4ea1-b927-58643c6692f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/data/datasets/leash-BELKA/AIS-UNK-Tokens.pickle', 'wb') as f:\n",
    "#     pickle.dump(UNK_tokens, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eed484-302a-46f8-8458-83726e37cc30",
   "metadata": {},
   "source": [
    "### Test data AIS tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff80862e-8a7a-429b-9b0d-0c7bdb55d374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1674896/1674896 [01:37<00:00, 17207.26it/s]\n"
     ]
    }
   ],
   "source": [
    "with multiprocessing.Pool(processes=32) as pool:\n",
    "    test_ais_encoded = list(tqdm(pool.imap(encode_smile, smiles_test), total=len(smiles_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b61db3-7954-48de-94ba-d48e9290e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ais_encoded = np.stack(test_ais_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a878b8-c2b7-4b00-87ec-33b393a84e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1674896, 150)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ais_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe89776-4d77-4fe6-8977-6f110e1cd1be",
   "metadata": {},
   "source": [
    "### Base test parquet, UNK = random noise vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccfcd067-df1a-4c07-883f-cde80aa859a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_base = pd.DataFrame(test_ais_encoded, columns = [f'token{i}' for i in range(150)])\n",
    "test_base.to_parquet('/data/datasets/leash-BELKA/test_ais_tokenized_base.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927db9d5-f320-4aff-a820-b21c06114a5f",
   "metadata": {},
   "source": [
    "### UNK = 221 test parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d7733ff-8b7c-422c-8d0d-425d29fa36d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for unk in UNK_tokens:\n",
    "    test_ais_encoded[test_ais_encoded == unk] = 221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c56f10b-fe31-4463-b9a2-9fff572b67b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(test_ais_encoded == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b48ea41-3763-48c6-9813-f8c0bc941a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unk_221 = pd.DataFrame(test_ais_encoded, columns = [f'token{i}' for i in range(150)])\n",
    "test_unk_221.to_parquet('/data/datasets/leash-BELKA/test_ais_tokenized_unk=221.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff68445a-f5c6-4b16-9924-35183ab91740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3534a87-dcef-4de4-8c20-f0ef322c335a",
   "metadata": {},
   "source": [
    "### Replace UNK tokens to other tokens using similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b84f3c65-51df-4380-baac-93653c7b4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "for k, v in ais_tokens_dict.items():\n",
    "    a[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94df02f4-f58e-4ccc-83ce-35debfa1cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_result_dict = {}\n",
    "for t in UNK_tokens:\n",
    "    cur_token = a[t]\n",
    "    sim_max = 0\n",
    "    idx = -1\n",
    "    for i in range(1, 221):\n",
    "        if i != t:\n",
    "            sim = atomInSmiles.similarity(cur_token, a[i])\n",
    "            if sim > sim_max:\n",
    "                sim_max = sim\n",
    "                idx = i\n",
    "    sim_result_dict[t] = (idx, sim_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f163c8a8-b7aa-4f5c-a406-c4474b50d09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{31: (-1, 0),\n",
       " 50: (-1, 0),\n",
       " 53: (-1, 0),\n",
       " 64: (-1, 0),\n",
       " 65: (-1, 0),\n",
       " 80: (-1, 0),\n",
       " 91: (-1, 0),\n",
       " 98: (-1, 0),\n",
       " 99: (-1, 0),\n",
       " 137: (-1, 0),\n",
       " 151: (-1, 0),\n",
       " 160: (-1, 0),\n",
       " 186: (-1, 0),\n",
       " 193: (-1, 0)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "033f501e-31ab-4aed-b395-9840937c92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_token_dict = {'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n",
    "                 '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25, '=': 26,\n",
    "                 '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 36}\n",
    "\n",
    "def tokenize_char(smiles):\n",
    "    smiles = remove_dy(smiles)\n",
    "    tmp = [ch_token_dict[tk] for tk in smiles]\n",
    "    assert len(tmp) <= MAX_LEN\n",
    "    tmp = tmp + [PAD]*(MAX_LEN-len(tmp))\n",
    "    return np.array(tmp).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27cdea4c-aeed-4b03-9cb0-95d630f49120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33  8  8 17 26 28 19 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "smiles = 'NCC(=O)O'\n",
    "\n",
    "print(tokenize_char(smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6041b27-b72d-4d22-8426-cc645d3bf4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1674896/1674896 [00:51<00:00, 32604.04it/s]\n"
     ]
    }
   ],
   "source": [
    "with multiprocessing.Pool(processes=32) as pool:\n",
    "    test_ch_encoded = list(tqdm(pool.imap(tokenize_char, smiles_test), total=len(smiles_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14cf4363-f691-47ab-bfc5-f9d31388f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ch_encoded = np.stack(test_ch_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51dac7d8-b23c-49a4-bc8b-e31714f83322",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ch_encoded = pd.DataFrame(test_ch_encoded, columns = [f'token{i}' for i in range(150)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a7c9890-6b52-4730-a500-78be27766013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token0</th>\n",
       "      <th>token1</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "      <th>token4</th>\n",
       "      <th>token5</th>\n",
       "      <th>token6</th>\n",
       "      <th>token7</th>\n",
       "      <th>token8</th>\n",
       "      <th>token9</th>\n",
       "      <th>...</th>\n",
       "      <th>token140</th>\n",
       "      <th>token141</th>\n",
       "      <th>token142</th>\n",
       "      <th>token143</th>\n",
       "      <th>token144</th>\n",
       "      <th>token145</th>\n",
       "      <th>token146</th>\n",
       "      <th>token147</th>\n",
       "      <th>token148</th>\n",
       "      <th>token149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   token0  token1  token2  token3  token4  token5  token6  token7  token8   \n",
       "0       8      22       8       8       8       8      29       8       3  \\\n",
       "1       8      22       8       8       8       8      29       8       3   \n",
       "2       8      22       8       8       8       8      29       8       3   \n",
       "\n",
       "   token9  ...  token140  token141  token142  token143  token144  token145   \n",
       "0       5  ...         0         0         0         0         0         0  \\\n",
       "1       5  ...         0         0         0         0         0         0   \n",
       "2       5  ...         0         0         0         0         0         0   \n",
       "\n",
       "   token146  token147  token148  token149  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "\n",
       "[3 rows x 150 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ch_encoded.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c9f0ad3-47ca-4f24-a58d-2f233a805c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ch_encoded.to_parquet('/data/datasets/leash-BELKA/test_ch_tokenized.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3d844e8-0b5d-4cce-a2dd-f51e39cb4ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83653268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83653268/83653268 [42:59<00:00, 32424.14it/s]\n"
     ]
    }
   ],
   "source": [
    "smiles_train = df_train['molecule_smiles'].values\n",
    "print(len(smiles_train))\n",
    "\n",
    "with multiprocessing.Pool(processes=32) as pool:\n",
    "    train_ch_encoded = list(tqdm(pool.imap(tokenize_char, smiles_train), total=len(smiles_train)))\n",
    "train_ch_encoded = np.stack(train_ch_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1546b465-2abb-4e04-b3cf-a5b3db572cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ch_encoded = pd.DataFrame(train_ch_encoded, columns = [f'token{i}' for i in range(150)])\n",
    "train_ch_encoded['binds_BRD4'] = df_train['binds_BRD4'].values\n",
    "train_ch_encoded['binds_HSA'] = df_train['binds_HSA'].values\n",
    "train_ch_encoded['binds_sEH'] = df_train['binds_sEH'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2a89f72-a393-4c00-a1e3-0a3a8b2884bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token0</th>\n",
       "      <th>token1</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "      <th>token4</th>\n",
       "      <th>token5</th>\n",
       "      <th>token6</th>\n",
       "      <th>token7</th>\n",
       "      <th>token8</th>\n",
       "      <th>token9</th>\n",
       "      <th>...</th>\n",
       "      <th>token143</th>\n",
       "      <th>token144</th>\n",
       "      <th>token145</th>\n",
       "      <th>token146</th>\n",
       "      <th>token147</th>\n",
       "      <th>token148</th>\n",
       "      <th>token149</th>\n",
       "      <th>binds_BRD4</th>\n",
       "      <th>binds_HSA</th>\n",
       "      <th>binds_sEH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   token0  token1  token2  token3  token4  token5  token6  token7  token8   \n",
       "0       8       8      17       8      33      12      27      35      12  \\\n",
       "1       8      35      27      35      12      12      12      27      29   \n",
       "2      28      26       8      17      33      19      29       8       3   \n",
       "3       8      12      27      10      12      17       8      19      12   \n",
       "\n",
       "   token9  ...  token143  token144  token145  token146  token147  token148   \n",
       "0      17  ...         0         0         0         0         0         0  \\\n",
       "1       8  ...         0         0         0         0         0         0   \n",
       "2       5  ...         0         0         0         0         0         0   \n",
       "3      17  ...         0         0         0         0         0         0   \n",
       "\n",
       "   token149  binds_BRD4  binds_HSA  binds_sEH  \n",
       "0         0           0          0          0  \n",
       "1         0           0          0          0  \n",
       "2         0           0          0          0  \n",
       "3         0           0          0          0  \n",
       "\n",
       "[4 rows x 153 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ch_encoded.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e955ab88-024e-497f-8a08-a6690edac2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ch_encoded.to_parquet('/data/datasets/leash-BELKA/random_stratified_split/train_ch_tokenized.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02ea7302-75ba-4616-b336-af3f2b3d811d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14762342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14762342/14762342 [07:47<00:00, 31589.81it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_smiles = df_valid['molecule_smiles'].values\n",
    "print(len(valid_smiles))\n",
    "\n",
    "with multiprocessing.Pool(processes=32) as pool:\n",
    "    valid_ch_encoded = list(tqdm(pool.imap(tokenize_char, valid_smiles), total=len(valid_smiles)))\n",
    "valid_ch_encoded = np.stack(valid_ch_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e54821a-120e-4c17-a640-48f46cb1a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ch_encoded = pd.DataFrame(valid_ch_encoded, columns = [f'token{i}' for i in range(150)])\n",
    "valid_ch_encoded['binds_BRD4'] = df_valid['binds_BRD4'].values\n",
    "valid_ch_encoded['binds_HSA'] = df_valid['binds_HSA'].values\n",
    "valid_ch_encoded['binds_sEH'] = df_valid['binds_sEH'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9382def-7a58-4e3c-94c7-33099722cce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token0</th>\n",
       "      <th>token1</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "      <th>token4</th>\n",
       "      <th>token5</th>\n",
       "      <th>token6</th>\n",
       "      <th>token7</th>\n",
       "      <th>token8</th>\n",
       "      <th>token9</th>\n",
       "      <th>...</th>\n",
       "      <th>token143</th>\n",
       "      <th>token144</th>\n",
       "      <th>token145</th>\n",
       "      <th>token146</th>\n",
       "      <th>token147</th>\n",
       "      <th>token148</th>\n",
       "      <th>token149</th>\n",
       "      <th>binds_BRD4</th>\n",
       "      <th>binds_HSA</th>\n",
       "      <th>binds_sEH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   token0  token1  token2  token3  token4  token5  token6  token7  token8   \n",
       "0       8      28      12      27      12      12      18      12      17  \\\n",
       "1       8       6       6       8       8      33      12      27      35   \n",
       "2       8      28      12      27      12      12      12      12      17   \n",
       "3       8      28       8      17      26      28      19      12      27   \n",
       "4       8      12      27      12      12      17      31       9      19   \n",
       "\n",
       "   token9  ...  token143  token144  token145  token146  token147  token148   \n",
       "0      12  ...         0         0         0         0         0         0  \\\n",
       "1      12  ...         0         0         0         0         0         0   \n",
       "2      36  ...         0         0         0         0         0         0   \n",
       "3      10  ...         0         0         0         0         0         0   \n",
       "4      12  ...         0         0         0         0         0         0   \n",
       "\n",
       "   token149  binds_BRD4  binds_HSA  binds_sEH  \n",
       "0         0           0          0          0  \n",
       "1         0           0          0          0  \n",
       "2         0           0          0          0  \n",
       "3         0           0          0          0  \n",
       "4         0           0          0          0  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ch_encoded.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50ae51c8-f0f1-4f98-8ce5-21a80bc8f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ch_encoded.to_parquet('/data/datasets/leash-BELKA/random_stratified_split/valid_ch_tokenized.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddff31bf-3a9a-483b-a810-1b5e5f08a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/datasets/leash-BELKA/AIS-UNK-Tokens.pickle', 'rb') as f:\n",
    "    unk = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d0ee2aac-05fd-4f69-bec6-2935b8390801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 50, 53, 64, 65, 80, 91, 98, 99, 137, 151, 160, 186, 193]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
